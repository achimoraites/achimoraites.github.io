import{S as n,i as a,s,A as t,j as e,m as o,o as p,p as c,v as r,r as i,w as l,q as u,e as k,t as h,k as g,c as d,a as m,g as f,d as b,n as y,b as _,f as v,D as w,E}from"../../chunks/vendor-12ce1488.js";import{B as T}from"../../chunks/blog-post-layout-e881a5a0.js";import"../../chunks/OpenGraph-f36c6cfd.js";import"../../chunks/ArticleCard-73ec8643.js";function P(n){let a,s,t,e,o,p,c,r,i,l,u,T,P,H,R,F,x,L,A,z,N,S,M,B,C,O,I,$,q,G,D,W,j,Y,K,U,J,V,Q,X,Z,nn,an,sn,tn,en,on,pn,cn,rn,ln,un,kn,hn,gn,dn,mn,fn,bn,yn,_n,vn,wn,En,Tn,Pn,Hn,Rn,Fn,xn,Ln,An,zn,Nn,Sn,Mn,Bn,Cn,On,In,$n,qn,Gn,Dn,Wn,jn,Yn,Kn,Un,Jn,Vn,Qn,Xn,Zn,na,aa,sa,ta,ea,oa,pa,ca,ra,ia,la,ua,ka,ha,ga,da,ma,fa,ba,ya,_a,va,wa,Ea,Ta,Pa,Ha,Ra,Fa,xa,La,Aa,za,Na,Sa,Ma,Ba,Ca,Oa,Ia,$a,qa,Ga,Da,Wa,ja,Ya,Ka,Ua,Ja,Va,Qa,Xa,Za,ns,as,ss,ts,es,os,ps,cs,rs,is,ls,us,ks,hs,gs,ds,ms,fs,bs,ys,_s,vs,ws,Es,Ts,Ps,Hs,Rs,Fs,xs,Ls,As,zs,Ns,Ss,Ms,Bs,Cs,Os,Is;return{c(){a=k("p"),s=h("Photo by "),t=k("a"),e=h("Artem Podrez"),o=h("  from "),p=k("a"),c=h("Pexels"),r=g(),i=k("br"),l=g(),u=k("p"),T=h("In this tutorial, we fine-tune a¬†"),P=k("strong"),H=h("RoBERTa"),R=h("¬†model¬†for topic classification using the Hugging Face Transformers and Datasets libraries. By the end of this tutorial, you will have a powerful fine-tuned model for classifying topics and published it to Hugging Face ü§ó for people to use."),F=g(),x=k("h2"),L=h("Prerequisites"),A=g(),z=k("p"),N=h("This article assumes you have a¬†"),S=k("a"),M=h("Hugging Face"),B=h("¬†ü§ó account and working Python, NLP, and Deep learning knowledge."),C=g(),O=k("ol"),I=k("li"),$=k("strong"),q=h("Understanding of¬†"),G=k("a"),D=h("machine learning concepts"),W=h(": A basic understanding of machine learning concepts, such as supervised learning, training, and evaluation."),j=g(),Y=k("li"),K=k("strong"),U=h("Python programming experience"),J=h("¬†will help follow the code presented in the tutorial."),V=g(),Q=k("li"),X=k("strong"),Z=h("Familiarity with deep learning and¬†"),nn=k("a"),an=h("natural language processing"),sn=h(": Some experience with deep learning and natural language processing (NLP) concepts, such as neural networks, word embeddings, and tokenization, will be beneficial for grasping the ideas presented in the tutorial."),tn=g(),en=k("li"),on=k("strong"),pn=h("Access to a Google Colab or Jupyter Notebook environment"),cn=h("."),rn=g(),ln=k("li"),un=k("strong"),kn=h("A Hugging Face account"),hn=h(": To publish your fine-tuned model to the Hugging Face Hub, you will need a Hugging Face account. If you do not already have an account, you can sign up for one at¬†"),gn=k("a"),dn=h("https://huggingface.co/join"),mn=h("."),fn=g(),bn=k("p"),yn=h("By meeting these prerequisites, you will be well-prepared to follow the tutorial and get the most out of it."),_n=g(),vn=k("hr"),wn=g(),En=k("h2"),Tn=h("Let‚Äôs get our hands dirty üòÅ"),Pn=g(),Hn=k("p"),Rn=h("We start by installing the dependencies."),Fn=g(),xn=k("pre"),Ln=g(),An=k("p"),zn=h("We then import the needed modules."),Nn=g(),Sn=k("pre"),Mn=g(),Bn=k("p"),Cn=h("We need to log in to Hugging Face by using a token."),On=g(),In=k("pre"),$n=g(),qn=k("p"),Gn=h("Let‚Äôs set some variables for easier configuration."),Dn=g(),Wn=k("pre"),jn=g(),Yn=k("h2"),Kn=h("Preprossessing"),Un=g(),Jn=k("p"),Vn=h("Next, we load our dataset and do some preprocessing."),Qn=g(),Xn=k("pre"),Zn=g(),na=k("h2"),aa=h("Set The Dataset Format"),sa=g(),ta=k("p"),ea=h("The¬†"),oa=k("code"),pa=h("set_format()"),ca=h("¬†function is used to specify the dataset format, making it compatible with PyTorch."),ra=g(),ia=k("p"),la=h("The¬†"),ua=k("code"),ka=h("columns"),ha=h("¬†argument lists the columns that should be included in the formatted dataset."),ga=g(),da=k("p"),ma=h("In this case, the columns are ‚Äúinput_ids‚Äù, ‚Äúattention_mask‚Äù, and ‚Äúlabel‚Äù."),fa=g(),ba=k("p"),ya=h("By setting the format and specifying the relevant columns, we prepare the datasets for use with the Hugging Face Trainer class, which requires PyTorch tensors as input."),_a=g(),va=k("pre"),wa=g(),Ea=k("p"),Ta=h("To make our model easier to use, we will create an¬†"),Pa=k("strong"),Ha=h("id2label"),Ra=h("¬†mapping. This will map the class ids to their labels."),Fa=g(),xa=k("p"),La=h("This makes it easier to interpret the model‚Äôs output during inference."),Aa=g(),za=k("pre"),Na=g(),Sa=k("h2"),Ma=h("Training and Evaluation"),Ba=g(),Ca=k("p"),Oa=h("Now, we will set up our training parameters, Hugging Face ü§ó repository, and Tensorboard."),Ia=g(),$a=k("pre"),qa=g(),Ga=k("p"),Da=h("We can start the training process by running:"),Wa=g(),ja=k("pre"),Ya=g(),Ka=k("p"),Ua=h("Evaluate the model:"),Ja=g(),Va=k("pre"),Qa=g(),Xa=k("hr"),Za=g(),ns=k("h2"),as=h("Publishing"),ss=g(),ts=k("p"),es=h("We are ready to publish our model to Hugging Face ü§ó"),os=g(),ps=k("pre"),cs=g(),rs=k("h2"),is=h("Test The Model"),ls=g(),us=k("p"),ks=h("At this point, our model should have been published and will be available for use. Let‚Äôs test it!"),hs=g(),gs=k("pre"),ds=g(),ms=k("h2"),fs=h("Congratulations"),bs=g(),ys=k("p"),_s=h("You have fine-tuned and published a RoBERTa model for text classification using Hugging Face ü§ó transformers and datasets libraries!"),vs=g(),ws=k("p"),Es=k("a"),Ts=k("strong"),Ps=h("For reference, here is my fine-tuned model on Hugging Face"),Hs=h(" ü§ó"),Rs=g(),Fs=k("p"),xs=k("a"),Ls=k("strong"),As=h("You can find the code here"),zs=g(),Ns=k("p"),Ss=h("Also published¬†"),Ms=k("a"),Bs=h("on medium"),Cs=g(),Os=k("p"),Is=h("Happy ü§ñ learning üòÄ!"),this.h()},l(n){a=d(n,"P",{});var k=m(a);s=f(k,"Photo by "),t=d(k,"A",{href:!0,rel:!0});var h=m(t);e=f(h,"Artem Podrez"),h.forEach(b),o=f(k,"  from "),p=d(k,"A",{href:!0,rel:!0});var g=m(p);c=f(g,"Pexels"),g.forEach(b),k.forEach(b),r=y(n),i=d(n,"BR",{}),l=y(n),u=d(n,"P",{});var _=m(u);T=f(_,"In this tutorial, we fine-tune a¬†"),P=d(_,"STRONG",{});var v=m(P);H=f(v,"RoBERTa"),v.forEach(b),R=f(_,"¬†model¬†for topic classification using the Hugging Face Transformers and Datasets libraries. By the end of this tutorial, you will have a powerful fine-tuned model for classifying topics and published it to Hugging Face ü§ó for people to use."),_.forEach(b),F=y(n),x=d(n,"H2",{});var w=m(x);L=f(w,"Prerequisites"),w.forEach(b),A=y(n),z=d(n,"P",{});var E=m(z);N=f(E,"This article assumes you have a¬†"),S=d(E,"A",{href:!0,rel:!0});var $s=m(S);M=f($s,"Hugging Face"),$s.forEach(b),B=f(E,"¬†ü§ó account and working Python, NLP, and Deep learning knowledge."),E.forEach(b),C=y(n),O=d(n,"OL",{});var qs=m(O);I=d(qs,"LI",{});var Gs=m(I);$=d(Gs,"STRONG",{});var Ds=m($);q=f(Ds,"Understanding of¬†"),G=d(Ds,"A",{href:!0,rel:!0});var Ws=m(G);D=f(Ws,"machine learning concepts"),Ws.forEach(b),Ds.forEach(b),W=f(Gs,": A basic understanding of machine learning concepts, such as supervised learning, training, and evaluation."),Gs.forEach(b),j=y(qs),Y=d(qs,"LI",{});var js=m(Y);K=d(js,"STRONG",{});var Ys=m(K);U=f(Ys,"Python programming experience"),Ys.forEach(b),J=f(js,"¬†will help follow the code presented in the tutorial."),js.forEach(b),V=y(qs),Q=d(qs,"LI",{});var Ks=m(Q);X=d(Ks,"STRONG",{});var Us=m(X);Z=f(Us,"Familiarity with deep learning and¬†"),nn=d(Us,"A",{href:!0,rel:!0});var Js=m(nn);an=f(Js,"natural language processing"),Js.forEach(b),Us.forEach(b),sn=f(Ks,": Some experience with deep learning and natural language processing (NLP) concepts, such as neural networks, word embeddings, and tokenization, will be beneficial for grasping the ideas presented in the tutorial."),Ks.forEach(b),tn=y(qs),en=d(qs,"LI",{});var Vs=m(en);on=d(Vs,"STRONG",{});var Qs=m(on);pn=f(Qs,"Access to a Google Colab or Jupyter Notebook environment"),Qs.forEach(b),cn=f(Vs,"."),Vs.forEach(b),rn=y(qs),ln=d(qs,"LI",{});var Xs=m(ln);un=d(Xs,"STRONG",{});var Zs=m(un);kn=f(Zs,"A Hugging Face account"),Zs.forEach(b),hn=f(Xs,": To publish your fine-tuned model to the Hugging Face Hub, you will need a Hugging Face account. If you do not already have an account, you can sign up for one at¬†"),gn=d(Xs,"A",{href:!0,rel:!0});var nt=m(gn);dn=f(nt,"https://huggingface.co/join"),nt.forEach(b),mn=f(Xs,"."),Xs.forEach(b),qs.forEach(b),fn=y(n),bn=d(n,"P",{});var at=m(bn);yn=f(at,"By meeting these prerequisites, you will be well-prepared to follow the tutorial and get the most out of it."),at.forEach(b),_n=y(n),vn=d(n,"HR",{}),wn=y(n),En=d(n,"H2",{});var st=m(En);Tn=f(st,"Let‚Äôs get our hands dirty üòÅ"),st.forEach(b),Pn=y(n),Hn=d(n,"P",{});var tt=m(Hn);Rn=f(tt,"We start by installing the dependencies."),tt.forEach(b),Fn=y(n),xn=d(n,"PRE",{class:!0}),m(xn).forEach(b),Ln=y(n),An=d(n,"P",{});var et=m(An);zn=f(et,"We then import the needed modules."),et.forEach(b),Nn=y(n),Sn=d(n,"PRE",{class:!0}),m(Sn).forEach(b),Mn=y(n),Bn=d(n,"P",{});var ot=m(Bn);Cn=f(ot,"We need to log in to Hugging Face by using a token."),ot.forEach(b),On=y(n),In=d(n,"PRE",{class:!0}),m(In).forEach(b),$n=y(n),qn=d(n,"P",{});var pt=m(qn);Gn=f(pt,"Let‚Äôs set some variables for easier configuration."),pt.forEach(b),Dn=y(n),Wn=d(n,"PRE",{class:!0}),m(Wn).forEach(b),jn=y(n),Yn=d(n,"H2",{});var ct=m(Yn);Kn=f(ct,"Preprossessing"),ct.forEach(b),Un=y(n),Jn=d(n,"P",{});var rt=m(Jn);Vn=f(rt,"Next, we load our dataset and do some preprocessing."),rt.forEach(b),Qn=y(n),Xn=d(n,"PRE",{class:!0}),m(Xn).forEach(b),Zn=y(n),na=d(n,"H2",{});var it=m(na);aa=f(it,"Set The Dataset Format"),it.forEach(b),sa=y(n),ta=d(n,"P",{});var lt=m(ta);ea=f(lt,"The¬†"),oa=d(lt,"CODE",{});var ut=m(oa);pa=f(ut,"set_format()"),ut.forEach(b),ca=f(lt,"¬†function is used to specify the dataset format, making it compatible with PyTorch."),lt.forEach(b),ra=y(n),ia=d(n,"P",{});var kt=m(ia);la=f(kt,"The¬†"),ua=d(kt,"CODE",{});var ht=m(ua);ka=f(ht,"columns"),ht.forEach(b),ha=f(kt,"¬†argument lists the columns that should be included in the formatted dataset."),kt.forEach(b),ga=y(n),da=d(n,"P",{});var gt=m(da);ma=f(gt,"In this case, the columns are ‚Äúinput_ids‚Äù, ‚Äúattention_mask‚Äù, and ‚Äúlabel‚Äù."),gt.forEach(b),fa=y(n),ba=d(n,"P",{});var dt=m(ba);ya=f(dt,"By setting the format and specifying the relevant columns, we prepare the datasets for use with the Hugging Face Trainer class, which requires PyTorch tensors as input."),dt.forEach(b),_a=y(n),va=d(n,"PRE",{class:!0}),m(va).forEach(b),wa=y(n),Ea=d(n,"P",{});var mt=m(Ea);Ta=f(mt,"To make our model easier to use, we will create an¬†"),Pa=d(mt,"STRONG",{});var ft=m(Pa);Ha=f(ft,"id2label"),ft.forEach(b),Ra=f(mt,"¬†mapping. This will map the class ids to their labels."),mt.forEach(b),Fa=y(n),xa=d(n,"P",{});var bt=m(xa);La=f(bt,"This makes it easier to interpret the model‚Äôs output during inference."),bt.forEach(b),Aa=y(n),za=d(n,"PRE",{class:!0}),m(za).forEach(b),Na=y(n),Sa=d(n,"H2",{});var yt=m(Sa);Ma=f(yt,"Training and Evaluation"),yt.forEach(b),Ba=y(n),Ca=d(n,"P",{});var _t=m(Ca);Oa=f(_t,"Now, we will set up our training parameters, Hugging Face ü§ó repository, and Tensorboard."),_t.forEach(b),Ia=y(n),$a=d(n,"PRE",{class:!0}),m($a).forEach(b),qa=y(n),Ga=d(n,"P",{});var vt=m(Ga);Da=f(vt,"We can start the training process by running:"),vt.forEach(b),Wa=y(n),ja=d(n,"PRE",{class:!0}),m(ja).forEach(b),Ya=y(n),Ka=d(n,"P",{});var wt=m(Ka);Ua=f(wt,"Evaluate the model:"),wt.forEach(b),Ja=y(n),Va=d(n,"PRE",{class:!0}),m(Va).forEach(b),Qa=y(n),Xa=d(n,"HR",{}),Za=y(n),ns=d(n,"H2",{});var Et=m(ns);as=f(Et,"Publishing"),Et.forEach(b),ss=y(n),ts=d(n,"P",{});var Tt=m(ts);es=f(Tt,"We are ready to publish our model to Hugging Face ü§ó"),Tt.forEach(b),os=y(n),ps=d(n,"PRE",{class:!0}),m(ps).forEach(b),cs=y(n),rs=d(n,"H2",{});var Pt=m(rs);is=f(Pt,"Test The Model"),Pt.forEach(b),ls=y(n),us=d(n,"P",{});var Ht=m(us);ks=f(Ht,"At this point, our model should have been published and will be available for use. Let‚Äôs test it!"),Ht.forEach(b),hs=y(n),gs=d(n,"PRE",{class:!0}),m(gs).forEach(b),ds=y(n),ms=d(n,"H2",{});var Rt=m(ms);fs=f(Rt,"Congratulations"),Rt.forEach(b),bs=y(n),ys=d(n,"P",{});var Ft=m(ys);_s=f(Ft,"You have fine-tuned and published a RoBERTa model for text classification using Hugging Face ü§ó transformers and datasets libraries!"),Ft.forEach(b),vs=y(n),ws=d(n,"P",{});var xt=m(ws);Es=d(xt,"A",{href:!0,rel:!0});var Lt=m(Es);Ts=d(Lt,"STRONG",{});var At=m(Ts);Ps=f(At,"For reference, here is my fine-tuned model on Hugging Face"),At.forEach(b),Hs=f(Lt," ü§ó"),Lt.forEach(b),xt.forEach(b),Rs=y(n),Fs=d(n,"P",{});var zt=m(Fs);xs=d(zt,"A",{href:!0,rel:!0});var Nt=m(xs);Ls=d(Nt,"STRONG",{});var St=m(Ls);As=f(St,"You can find the code here"),St.forEach(b),Nt.forEach(b),zt.forEach(b),zs=y(n),Ns=d(n,"P",{});var Mt=m(Ns);Ss=f(Mt,"Also published¬†"),Ms=d(Mt,"A",{href:!0,rel:!0});var Bt=m(Ms);Bs=f(Bt,"on medium"),Bt.forEach(b),Mt.forEach(b),Cs=y(n),Os=d(n,"P",{});var Ct=m(Os);Is=f(Ct,"Happy ü§ñ learning üòÄ!"),Ct.forEach(b),this.h()},h(){_(t,"href","https://www.pexels.com/@artempodrez?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels"),_(t,"rel","nofollow"),_(p,"href","https://www.pexels.com/photo/a-woman-doing-an-experiment-5726788/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels"),_(p,"rel","nofollow"),_(S,"href","https://hackernoon.com/10-best-hugging-face-datasets-for-building-nlp-models?ref=hackernoon.com"),_(S,"rel","nofollow"),_(G,"href","https://hackernoon.com/machine-learning-algorithms-explained?ref=hackernoon.com"),_(G,"rel","nofollow"),_(nn,"href","https://hackernoon.com/what-is-natural-language-processing-a-brief-overview-wzm310l?ref=hackernoon.com"),_(nn,"rel","nofollow"),_(gn,"href","https://huggingface.co/join?ref=hackernoon.com"),_(gn,"rel","nofollow"),_(xn,"class","language-python"),_(Sn,"class","language-python"),_(In,"class","language-python"),_(Wn,"class","language-python"),_(Xn,"class","language-python"),_(va,"class","language-python"),_(za,"class","language-python"),_($a,"class","language-python"),_(ja,"class","language-python"),_(Va,"class","language-python"),_(ps,"class","language-python"),_(gs,"class","language-python"),_(Es,"href","https://huggingface.co/achimoraites/roberta-base_ag_news"),_(Es,"rel","nofollow"),_(xs,"href","https://github.com/achimoraites/machine-learning-playground/blob/main/NLP/Text%20classification/RoBERTa_Finetuning.ipynb"),_(xs,"rel","nofollow"),_(Ms,"href","https://medium.com/@achillesmoraites/fine-tuning-roberta-for-topic-classification-with-hugging-face-transformers-and-datasets-library-c6f8432d0820"),_(Ms,"rel","nofollow")},m(n,k){v(n,a,k),w(a,s),w(a,t),w(t,e),w(a,o),w(a,p),w(p,c),v(n,r,k),v(n,i,k),v(n,l,k),v(n,u,k),w(u,T),w(u,P),w(P,H),w(u,R),v(n,F,k),v(n,x,k),w(x,L),v(n,A,k),v(n,z,k),w(z,N),w(z,S),w(S,M),w(z,B),v(n,C,k),v(n,O,k),w(O,I),w(I,$),w($,q),w($,G),w(G,D),w(I,W),w(O,j),w(O,Y),w(Y,K),w(K,U),w(Y,J),w(O,V),w(O,Q),w(Q,X),w(X,Z),w(X,nn),w(nn,an),w(Q,sn),w(O,tn),w(O,en),w(en,on),w(on,pn),w(en,cn),w(O,rn),w(O,ln),w(ln,un),w(un,kn),w(ln,hn),w(ln,gn),w(gn,dn),w(ln,mn),v(n,fn,k),v(n,bn,k),w(bn,yn),v(n,_n,k),v(n,vn,k),v(n,wn,k),v(n,En,k),w(En,Tn),v(n,Pn,k),v(n,Hn,k),w(Hn,Rn),v(n,Fn,k),v(n,xn,k),xn.innerHTML='<code class="language-python">!pip install transformers datasets huggingface_hub tensorboard<span class="token operator">==</span><span class="token number">2.11</span>\n!sudo apt<span class="token operator">-</span>get install git<span class="token operator">-</span>lfs <span class="token operator">-</span><span class="token operator">-</span>yes</code>',v(n,Ln,k),v(n,An,k),w(An,zn),v(n,Nn,k),v(n,Sn,k),Sn.innerHTML='<code class="language-python"><span class="token keyword">import</span> torch\n<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset\n<span class="token keyword">from</span> transformers <span class="token keyword">import</span> <span class="token punctuation">(</span>\n    RobertaTokenizerFast<span class="token punctuation">,</span>\n    RobertaForSequenceClassification<span class="token punctuation">,</span>\n    TrainingArguments<span class="token punctuation">,</span>\n    Trainer<span class="token punctuation">,</span>\n    AutoConfig<span class="token punctuation">,</span>\n<span class="token punctuation">)</span>\n<span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> HfFolder<span class="token punctuation">,</span> notebook_login</code>',v(n,Mn,k),v(n,Bn,k),w(Bn,Cn),v(n,On,k),v(n,In,k),In.innerHTML='<code class="language-python">notebook_login<span class="token punctuation">(</span><span class="token punctuation">)</span></code>',v(n,$n,k),v(n,qn,k),w(qn,Gn),v(n,Dn,k),v(n,Wn,k),Wn.innerHTML='<code class="language-python">model_id <span class="token operator">=</span> <span class="token string">"roberta-base"</span>\ndataset_id <span class="token operator">=</span> <span class="token string">"ag_news"</span>\n<span class="token comment"># relace the value with your model: ex &lt;hugging-face-user>/&lt;model-name></span>\nrepository_id <span class="token operator">=</span> <span class="token string">"achimoraites/roberta-base_ag_news"</span></code>',v(n,jn,k),v(n,Yn,k),w(Yn,Kn),v(n,Un,k),v(n,Jn,k),w(Jn,Vn),v(n,Qn,k),v(n,Xn,k),Xn.innerHTML='<code class="language-python"><span class="token comment"># Load dataset</span>\ndataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>dataset_id<span class="token punctuation">)</span>\n\n<span class="token comment"># Training and testing datasets</span>\ntrain_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">\'train\'</span><span class="token punctuation">]</span>\ntest_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shard<span class="token punctuation">(</span>num_shards<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>\n\n<span class="token comment"># Validation dataset</span>\nval_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">\'test\'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shard<span class="token punctuation">(</span>num_shards<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>\n\n<span class="token comment"># Preprocessing</span>\ntokenizer <span class="token operator">=</span> RobertaTokenizerFast<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>\n\n<span class="token comment"># This function tokenizes the input text using the RoBERTa tokenizer. </span>\n<span class="token comment"># It applies padding and truncation to ensure that all sequences have the same length (256 tokens).</span>\n<span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>\n    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span>\n\ntrain_dataset <span class="token operator">=</span> train_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>\nval_dataset <span class="token operator">=</span> val_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>val_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>\ntest_dataset <span class="token operator">=</span> test_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span></code>',v(n,Zn,k),v(n,na,k),w(na,aa),v(n,sa,k),v(n,ta,k),w(ta,ea),w(ta,oa),w(oa,pa),w(ta,ca),v(n,ra,k),v(n,ia,k),w(ia,la),w(ia,ua),w(ua,ka),w(ia,ha),v(n,ga,k),v(n,da,k),w(da,ma),v(n,fa,k),v(n,ba,k),w(ba,ya),v(n,_a,k),v(n,va,k),va.innerHTML='<code class="language-python"><span class="token comment"># Set dataset format</span>\ntrain_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>\nval_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>\ntest_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code>',v(n,wa,k),v(n,Ea,k),w(Ea,Ta),w(Ea,Pa),w(Pa,Ha),w(Ea,Ra),v(n,Fa,k),v(n,xa,k),w(xa,La),v(n,Aa,k),v(n,za,k),za.innerHTML='<code class="language-python"><span class="token comment"># We will need this to directly output the class names when using the pipeline without mapping the labels later.</span>\n<span class="token comment"># Extract the number of classes and their names</span>\nnum_labels <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">\'train\'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">\'label\'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>num_classes\nclass_names <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>names\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"number of labels: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>num_labels<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"the labels: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>class_names<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>\n\n<span class="token comment"># Create an id2label mapping</span>\nid2label <span class="token operator">=</span> <span class="token punctuation">&#123;</span>i<span class="token punctuation">:</span> label <span class="token keyword">for</span> i<span class="token punctuation">,</span> label <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>class_names<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>\n\n<span class="token comment"># Update the model\'s configuration with the id2label mapping</span>\nconfig <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>\nconfig<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"id2label"</span><span class="token punctuation">:</span> id2label<span class="token punctuation">&#125;</span><span class="token punctuation">)</span></code>',v(n,Na,k),v(n,Sa,k),w(Sa,Ma),v(n,Ba,k),v(n,Ca,k),w(Ca,Oa),v(n,Ia,k),v(n,$a,k),$a.innerHTML='<code class="language-python"><span class="token comment"># Model</span>\nmodel <span class="token operator">=</span> RobertaForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>\n\n<span class="token comment"># TrainingArguments</span>\ntraining_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>\n    output_dir<span class="token operator">=</span>repository_id<span class="token punctuation">,</span>\n    num_train_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>\n    per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>\n    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>\n    evaluation_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>\n    logging_dir<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>repository_id<span class="token punctuation">&#125;</span></span><span class="token string">/logs"</span></span><span class="token punctuation">,</span>\n    logging_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>\n    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>\n    learning_rate<span class="token operator">=</span><span class="token number">5e-5</span><span class="token punctuation">,</span>\n    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>\n    warmup_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>\n    save_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>\n    load_best_model_at_end<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>\n    save_total_limit<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>\n    report_to<span class="token operator">=</span><span class="token string">"tensorboard"</span><span class="token punctuation">,</span>\n    push_to_hub<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>\n    hub_strategy<span class="token operator">=</span><span class="token string">"every_save"</span><span class="token punctuation">,</span>\n    hub_model_id<span class="token operator">=</span>repository_id<span class="token punctuation">,</span>\n    hub_token<span class="token operator">=</span>HfFolder<span class="token punctuation">.</span>get_token<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n<span class="token punctuation">)</span>\n\n<span class="token comment"># Trainer</span>\ntrainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>\n    model<span class="token operator">=</span>model<span class="token punctuation">,</span>\n    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>\n    train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>\n    eval_dataset<span class="token operator">=</span>val_dataset<span class="token punctuation">,</span>\n<span class="token punctuation">)</span></code>',v(n,qa,k),v(n,Ga,k),w(Ga,Da),v(n,Wa,k),v(n,ja,k),ja.innerHTML='<code class="language-python"><span class="token comment"># Fine-tune the model</span>\ntrainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></code>',v(n,Ya,k),v(n,Ka,k),w(Ka,Ua),v(n,Ja,k),v(n,Va,k),Va.innerHTML='<code class="language-python"><span class="token comment"># Evaluate the model</span>\ntrainer<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span><span class="token punctuation">)</span></code>',v(n,Qa,k),v(n,Xa,k),v(n,Za,k),v(n,ns,k),w(ns,as),v(n,ss,k),v(n,ts,k),w(ts,es),v(n,os,k),v(n,ps,k),ps.innerHTML='<code class="language-python"><span class="token comment"># Save our tokenizer and create a model card</span>\ntokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>repository_id<span class="token punctuation">)</span>\ntrainer<span class="token punctuation">.</span>create_model_card<span class="token punctuation">(</span><span class="token punctuation">)</span>\n<span class="token comment"># Push the results to the hub</span>\ntrainer<span class="token punctuation">.</span>push_to_hub<span class="token punctuation">(</span><span class="token punctuation">)</span></code>',v(n,cs,k),v(n,rs,k),w(rs,is),v(n,ls,k),v(n,us,k),w(us,ks),v(n,hs,k),v(n,gs,k),gs.innerHTML='<code class="language-python"><span class="token comment"># TEST MODEL</span>\n\n<span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline\n\nclassifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">\'text-classification\'</span><span class="token punctuation">,</span>repository_id<span class="token punctuation">)</span>\n\ntext <span class="token operator">=</span> <span class="token string">"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his innocence and vowing: quot;After the crucifixion comes the resurrection. quot; .."</span>\nresult <span class="token operator">=</span> classifier<span class="token punctuation">(</span>text<span class="token punctuation">)</span>\n\npredicted_label <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span>\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Predicted label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>predicted_label<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></code>',v(n,ds,k),v(n,ms,k),w(ms,fs),v(n,bs,k),v(n,ys,k),w(ys,_s),v(n,vs,k),v(n,ws,k),w(ws,Es),w(Es,Ts),w(Ts,Ps),w(Es,Hs),v(n,Rs,k),v(n,Fs,k),w(Fs,xs),w(xs,Ls),w(Ls,As),v(n,zs,k),v(n,Ns,k),w(Ns,Ss),w(Ns,Ms),w(Ms,Bs),v(n,Cs,k),v(n,Os,k),w(Os,Is)},p:E,d(n){n&&b(a),n&&b(r),n&&b(i),n&&b(l),n&&b(u),n&&b(F),n&&b(x),n&&b(A),n&&b(z),n&&b(C),n&&b(O),n&&b(fn),n&&b(bn),n&&b(_n),n&&b(vn),n&&b(wn),n&&b(En),n&&b(Pn),n&&b(Hn),n&&b(Fn),n&&b(xn),n&&b(Ln),n&&b(An),n&&b(Nn),n&&b(Sn),n&&b(Mn),n&&b(Bn),n&&b(On),n&&b(In),n&&b($n),n&&b(qn),n&&b(Dn),n&&b(Wn),n&&b(jn),n&&b(Yn),n&&b(Un),n&&b(Jn),n&&b(Qn),n&&b(Xn),n&&b(Zn),n&&b(na),n&&b(sa),n&&b(ta),n&&b(ra),n&&b(ia),n&&b(ga),n&&b(da),n&&b(fa),n&&b(ba),n&&b(_a),n&&b(va),n&&b(wa),n&&b(Ea),n&&b(Fa),n&&b(xa),n&&b(Aa),n&&b(za),n&&b(Na),n&&b(Sa),n&&b(Ba),n&&b(Ca),n&&b(Ia),n&&b($a),n&&b(qa),n&&b(Ga),n&&b(Wa),n&&b(ja),n&&b(Ya),n&&b(Ka),n&&b(Ja),n&&b(Va),n&&b(Qa),n&&b(Xa),n&&b(Za),n&&b(ns),n&&b(ss),n&&b(ts),n&&b(os),n&&b(ps),n&&b(cs),n&&b(rs),n&&b(ls),n&&b(us),n&&b(hs),n&&b(gs),n&&b(ds),n&&b(ms),n&&b(bs),n&&b(ys),n&&b(vs),n&&b(ws),n&&b(Rs),n&&b(Fs),n&&b(zs),n&&b(Ns),n&&b(Cs),n&&b(Os)}}}function H(n){let a,s;const k=[R];let h={$$slots:{default:[P]},$$scope:{ctx:n}};for(let e=0;e<k.length;e+=1)h=t(h,k[e]);return a=new T({props:h}),{c(){e(a.$$.fragment)},l(n){o(a.$$.fragment,n)},m(n,t){p(a,n,t),s=!0},p(n,[s]){const t=0&s?c(k,[u(R)]):{};1&s&&(t.$$scope={dirty:s,ctx:n}),a.$set(t)},i(n){s||(r(a.$$.fragment,n),s=!0)},o(n){i(a.$$.fragment,n),s=!1},d(n){l(a,n)}}}const R={author:"achimoraites",date:"Fri Mar 24 2023 00:00:00 GMT+0100",excerpt:"In This tutorial, we fine-tune a RoBERTa model for topic classification using the Hugging Face Transformers and Datasets libraries.",image:"https://images.pexels.com/photos/4050347/pexels-photo-4050347.jpeg?auto=compress&cs=tinysrgb&w=640&h=427&dpr=1",tags:["machine learning","python","nlp"],slug:"fine-tuning-roberta-for-topic-classification",subtitle:"In This tutorial, we fine-tune a RoBERTa model for topic classification using the Hugging Face Transformers and Datasets libraries.",title:"Fine-Tuning RoBERTa for Topic Classification",url:"https://achimoraites.github.io/blog/fine-tuning-roberta-for-topic-classification"},F=!0;export default class extends n{constructor(n){super(),a(this,n,null,H,s,{})}}export{R as metadata,F as prerender};
