<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="/favicon.ico" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Fine-Tuning RoBERTa for Topic Classification</title><meta name="keywords" content="web development, achilles moraites,machine learning,python,nlp" data-svelte="svelte-11c4p6r"><meta name="description" content="In This tutorial, we fine-tune a RoBERTa model for topic classification using the Hugging Face Transformers and Datasets libraries." data-svelte="svelte-11c4p6r"><meta name="author" content="Achilles Moraites" data-svelte="svelte-11c4p6r"><meta property="og:image" content="https://images.pexels.com/photos/4050347/pexels-photo-4050347.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=640&amp;h=427&amp;dpr=1" data-svelte="svelte-11c4p6r"><meta property="og:description" content="In This tutorial, we fine-tune a RoBERTa model for topic classification using the Hugging Face Transformers and Datasets libraries." data-svelte="svelte-11c4p6r"><meta property="og:title" content="Fine-Tuning RoBERTa for Topic Classification" data-svelte="svelte-11c4p6r"><meta property="og:type" content="article" data-svelte="svelte-11c4p6r"><meta property="og:url" data-svelte="svelte-11c4p6r"><meta property="og:article:author" content="Achilles Moraites" data-svelte="svelte-11c4p6r">
		<meta property="og:article:tag" content="machine learning,python,nlp" data-svelte="svelte-11c4p6r">
		<meta property="og:article:published_time" content="Fri Mar 24 2023 00:00:00 GMT+0100" data-svelte="svelte-11c4p6r"><meta name="twitter:card" content="summary" data-svelte="svelte-11c4p6r"><meta name="twitter:site" content="@achimoraites" data-svelte="svelte-11c4p6r"><meta name="twitter:creator" content="@achimoraites" data-svelte="svelte-11c4p6r"><meta name="twitter:image" content="https://images.pexels.com/photos/4050347/pexels-photo-4050347.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=640&amp;h=427&amp;dpr=1" data-svelte="svelte-11c4p6r"><link rel="preload" as="image" href="https://images.pexels.com/photos/4050347/pexels-photo-4050347.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=640&amp;h=427&amp;dpr=1" data-svelte="svelte-11c4p6r">

		

		<link rel="modulepreload" href="/./app/start-a30a3f06.js">
		<link rel="modulepreload" href="/./app/chunks/vendor-12ce1488.js">
		<link rel="modulepreload" href="/./app/pages/$layout.svelte-4947f72f.js">
		<link rel="modulepreload" href="/./app/pages/blog/fine-tuning-roberta-for-topic-classification.md-a4382780.js">
		<link rel="modulepreload" href="/./app/chunks/blog-post-layout-e881a5a0.js">
		<link rel="modulepreload" href="/./app/chunks/OpenGraph-f36c6cfd.js">
		<link rel="modulepreload" href="/./app/chunks/ArticleCard-73ec8643.js">
		<link rel="stylesheet" href="/./app/assets/start-0826e215.css">
		<link rel="stylesheet" href="/./app/assets/pages/$layout.svelte-00127a52.css">
		<link rel="stylesheet" href="/./app/assets/ArticleCard-c8a6b54e.css">

		<script type="module">
			import { start } from "/./app/start-a30a3f06.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":"/."},
				session: (function(a,b,c,d,e,f,g,h,i,j){return {posts:[{author:a,date:"Fri Mar 24 2023 00:00:00 GMT+0100",excerpt:e,image:"https:\u002F\u002Fimages.pexels.com\u002Fphotos\u002F4050347\u002Fpexels-photo-4050347.jpeg?auto=compress&cs=tinysrgb&w=640&h=427&dpr=1",tags:["machine learning","python","nlp"],slug:"fine-tuning-roberta-for-topic-classification",subtitle:e,title:"Fine-Tuning RoBERTa for Topic Classification",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Ffine-tuning-roberta-for-topic-classification",uri:"\u002Fblog\u002Ffine-tuning-roberta-for-topic-classification",filename:"fine-tuning-roberta-for-topic-classification.md"},{author:a,date:"Tue Jun 08 2021 21:57:00 GMT+0200",excerpt:"Here there are some cool tricks you can do to improve your Github Actions",image:"https:\u002F\u002Fimages.pexels.com\u002Fphotos\u002F1181287\u002Fpexels-photo-1181287.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=225&w=400",tags:["ci",f],slug:"github-actions-tips-and-tricks",title:"Github actions tweaks",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Fgithub-actions-tips-and-tricks",uri:"\u002Fblog\u002Fgithub-actions-tips-and-tricks",filename:"github-actions-tips-and-tricks.md"},{author:a,date:"Sun May 10 2020 00:00:00 GMT+0300",excerpt:"Crafting high quality Angular apps requires to have knowledge of how to use the different types of modules to ensure readability, performance and scalability.",tags:[b,c,d],image:"https:\u002F\u002Fimages.pexels.com\u002Fphotos\u002F171198\u002Fpexels-photo-171198.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=225&w=400",slug:"angular-types-of-modules",subtitle:"Improve the scalabillity of your Angular project by using the module type pattern!",title:"Types of Modules in Angular",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Fangular-types-of-modules",uri:"\u002Fblog\u002Fangular-types-of-modules",filename:"angular-types-of-modules.md"},{author:a,date:g,excerpt:"When testing our apps there are times when we need to control things that are beyond our control, like the window object",image:"https:\u002F\u002Fimages.pexels.com\u002Fphotos\u002F5726788\u002Fpexels-photo-5726788.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=255&w=400",tags:[c,d,b,"testing"],slug:"advanced-angular-testing-using-jasmine",subtitle:h,title:"Angular Testing with Jasmine",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Fadvanced-angular-testing-using-jasmine",uri:"\u002Fblog\u002Fadvanced-angular-testing-using-jasmine",filename:"advanced-angular-testing-using-jasmine.md"},{author:a,date:g,excerpt:"Component inheritance can be useful when you have duplicated\u002Fshared code between your components.",tags:[b,c,i,d],slug:"angular-component-inheritance",subtitle:h,title:"Angular Component inheritance",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Fangular-component-inheritance",uri:"\u002Fblog\u002Fangular-component-inheritance",filename:"angular-component-inheritance.md"},{author:a,date:"Wed Jan 29 2020 00:00:00 GMT+0200",excerpt:j,image:"https:\u002F\u002Fimages.pexels.com\u002Fphotos\u002F1701206\u002Fpexels-photo-1701206.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=225&w=400",tags:[b,i,f],slug:"typescript-tips-and-tricks",subtitle:j,title:"Typescript Tips and Tricks",url:"https:\u002F\u002Fachimoraites.github.io\u002Fblog\u002Ftypescript-tips-and-tricks",uri:"\u002Fblog\u002Ftypescript-tips-and-tricks",filename:"typescript-tips-and-tricks.md"}]}}("achimoraites","typescript","angular","javascript","In This tutorial, we fine-tune a RoBERTa model for topic classification using the Hugging Face Transformers and Datasets libraries.","productivity","Fri May 08 2020 00:00:00 GMT+0300","Improve the scalabillity of your Angular project by using inheritance!","oop","Here are two Typescript tricks to make your life as a software developer easier.")),
				host: location.host,
				route: true,
				spa: false,
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/./app/pages/$layout.svelte-4947f72f.js"),
						import("/./app/pages/blog/fine-tuning-roberta-for-topic-classification.md-a4382780.js")
					],
					page: {
						host: location.host, // TODO this is redundant
						path: "/blog/fine-tuning-roberta-for-topic-classification",
						query: new URLSearchParams(""),
						params: {}
					}
				}
			});
		</script>
	</head>
	<body>
		<div id="svelte">


<nav class="header top-nav"><div class="header__container"><div class="brand"><a class="brand__link" href="/blog">Achilles Moraites</a></div>

<div class="burger-menu"><button id="nav-toggle" class="burger-menu__button"><svg width="12" height="12" class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><title>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z"></path></svg></button></div>

<div class="navlinks hidden-bar" id="nav-content"><ul class="navlinks__list list-reset"><li class="navlinks__list-element"><a class="navlinks__default" href="/"><span>Home</span></a>
			</li><li class="navlinks__list-element"><a class="navlinks__default" href="/blog"><span>Blog</span></a>
			</li></ul></div></div></nav>
<main class="circuit-bg svelte-1xdevn2">

<article class="blog-post"><h1 style="font-size: 3rem;">Fine-Tuning RoBERTa for Topic Classification</h1>
	<div class="tags"><div class="article-tags"><a href="/blog"><span class="machine learning">#machine learning</span></a><a href="/blog"><span class="python">#python</span></a><a href="/blog"><span class="nlp">#nlp</span></a></div></div>
	<nav class="social-share float-right relative bottom-5 right-2 md:right-1 lg:right-0" arial-label="share this post"><ul class="flex flex-row"><li class="ml-4"><a arial-label="share on linkedin" href="https://www.linkedin.com/sharing/share-offsite/?url=https://achimoraites.github.io/blog/fine-tuning-roberta-for-topic-classification"><img width="24" height="24" title="share on linkedin" alt="share on linkedin image" src="/images/icons/linkedin.svg"></a>
	</li><li class="ml-4"><a arial-label="share on twitter" href="https://twitter.com/intent/tweet?text=https://achimoraites.github.io/blog/fine-tuning-roberta-for-topic-classification"><img width="24" height="24" title="share on twitter" alt="share on twitter image" src="/images/icons/twitter.svg"></a>
	</li></ul></nav>
	<p class="author">By <span style="font-weight: 600">achimoraites</span> <span>Mar 23, 2023</span></p>

	<img class="mt-4 mb-6" alt="Fine-Tuning RoBERTa for Topic Classification" src="https://images.pexels.com/photos/4050347/pexels-photo-4050347.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=640&amp;h=427&amp;dpr=1" width="800" height="450">
	<p>Photo by <a href="https://www.pexels.com/@artempodrez?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="nofollow">Artem Podrez</a>  from <a href="https://www.pexels.com/photo/a-woman-doing-an-experiment-5726788/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="nofollow">Pexels</a></p>
<br>
<p>In this tutorial, we fine-tune a <strong>RoBERTa</strong> model for topic classification using the Hugging Face Transformers and Datasets libraries. By the end of this tutorial, you will have a powerful fine-tuned model for classifying topics and published it to Hugging Face 🤗 for people to use.</p>
<h2>Prerequisites</h2>
<p>This article assumes you have a <a href="https://hackernoon.com/10-best-hugging-face-datasets-for-building-nlp-models?ref=hackernoon.com" rel="nofollow">Hugging Face</a> 🤗 account and working Python, NLP, and Deep learning knowledge.</p>
<ol><li><strong>Understanding of <a href="https://hackernoon.com/machine-learning-algorithms-explained?ref=hackernoon.com" rel="nofollow">machine learning concepts</a></strong>: A basic understanding of machine learning concepts, such as supervised learning, training, and evaluation.</li>
<li><strong>Python programming experience</strong> will help follow the code presented in the tutorial.</li>
<li><strong>Familiarity with deep learning and <a href="https://hackernoon.com/what-is-natural-language-processing-a-brief-overview-wzm310l?ref=hackernoon.com" rel="nofollow">natural language processing</a></strong>: Some experience with deep learning and natural language processing (NLP) concepts, such as neural networks, word embeddings, and tokenization, will be beneficial for grasping the ideas presented in the tutorial.</li>
<li><strong>Access to a Google Colab or Jupyter Notebook environment</strong>.</li>
<li><strong>A Hugging Face account</strong>: To publish your fine-tuned model to the Hugging Face Hub, you will need a Hugging Face account. If you do not already have an account, you can sign up for one at <a href="https://huggingface.co/join?ref=hackernoon.com" rel="nofollow">https://huggingface.co/join</a>.</li></ol>
<p>By meeting these prerequisites, you will be well-prepared to follow the tutorial and get the most out of it.</p>
<hr>
<h2>Let’s get our hands dirty 😁</h2>
<p>We start by installing the dependencies.</p>
<pre class="language-python"><code class="language-python">!pip install transformers datasets huggingface_hub tensorboard<span class="token operator">==</span><span class="token number">2.11</span>
!sudo apt<span class="token operator">-</span>get install git<span class="token operator">-</span>lfs <span class="token operator">-</span><span class="token operator">-</span>yes</code></pre>
<p>We then import the needed modules.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> <span class="token punctuation">(</span>
    RobertaTokenizerFast<span class="token punctuation">,</span>
    RobertaForSequenceClassification<span class="token punctuation">,</span>
    TrainingArguments<span class="token punctuation">,</span>
    Trainer<span class="token punctuation">,</span>
    AutoConfig<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> HfFolder<span class="token punctuation">,</span> notebook_login</code></pre>
<p>We need to log in to Hugging Face by using a token.</p>
<pre class="language-python"><code class="language-python">notebook_login<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<p>Let’s set some variables for easier configuration.</p>
<pre class="language-python"><code class="language-python">model_id <span class="token operator">=</span> <span class="token string">"roberta-base"</span>
dataset_id <span class="token operator">=</span> <span class="token string">"ag_news"</span>
<span class="token comment"># relace the value with your model: ex &lt;hugging-face-user>/&lt;model-name></span>
repository_id <span class="token operator">=</span> <span class="token string">"achimoraites/roberta-base_ag_news"</span></code></pre>
<h2>Preprossessing</h2>
<p>Next, we load our dataset and do some preprocessing.</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Load dataset</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>dataset_id<span class="token punctuation">)</span>

<span class="token comment"># Training and testing datasets</span>
train_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span>
test_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shard<span class="token punctuation">(</span>num_shards<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># Validation dataset</span>
val_dataset <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shard<span class="token punctuation">(</span>num_shards<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># Preprocessing</span>
tokenizer <span class="token operator">=</span> RobertaTokenizerFast<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>

<span class="token comment"># This function tokenizes the input text using the RoBERTa tokenizer. </span>
<span class="token comment"># It applies padding and truncation to ensure that all sequences have the same length (256 tokens).</span>
<span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> train_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
val_dataset <span class="token operator">=</span> val_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>val_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
test_dataset <span class="token operator">=</span> test_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h2>Set The Dataset Format</h2>
<p>The <code>set_format()</code> function is used to specify the dataset format, making it compatible with PyTorch.</p>
<p>The <code>columns</code> argument lists the columns that should be included in the formatted dataset.</p>
<p>In this case, the columns are “input_ids”, “attention_mask”, and “label”.</p>
<p>By setting the format and specifying the relevant columns, we prepare the datasets for use with the Hugging Face Trainer class, which requires PyTorch tensors as input.</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Set dataset format</span>
train_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
val_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
test_dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>
<p>To make our model easier to use, we will create an <strong>id2label</strong> mapping. This will map the class ids to their labels.</p>
<p>This makes it easier to interpret the model’s output during inference.</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># We will need this to directly output the class names when using the pipeline without mapping the labels later.</span>
<span class="token comment"># Extract the number of classes and their names</span>
num_labels <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>num_classes
class_names <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>names
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"number of labels: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>num_labels<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"the labels: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>class_names<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Create an id2label mapping</span>
id2label <span class="token operator">=</span> <span class="token punctuation">&#123;</span>i<span class="token punctuation">:</span> label <span class="token keyword">for</span> i<span class="token punctuation">,</span> label <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>class_names<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>

<span class="token comment"># Update the model's configuration with the id2label mapping</span>
config <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
config<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"id2label"</span><span class="token punctuation">:</span> id2label<span class="token punctuation">&#125;</span><span class="token punctuation">)</span></code></pre>
<h2>Training and Evaluation</h2>
<p>Now, we will set up our training parameters, Hugging Face 🤗 repository, and Tensorboard.</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Model</span>
model <span class="token operator">=</span> RobertaForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>

<span class="token comment"># TrainingArguments</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span>repository_id<span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    evaluation_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>
    logging_dir<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>repository_id<span class="token punctuation">&#125;</span></span><span class="token string">/logs"</span></span><span class="token punctuation">,</span>
    logging_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">5e-5</span><span class="token punctuation">,</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
    warmup_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
    save_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>
    load_best_model_at_end<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    save_total_limit<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    report_to<span class="token operator">=</span><span class="token string">"tensorboard"</span><span class="token punctuation">,</span>
    push_to_hub<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    hub_strategy<span class="token operator">=</span><span class="token string">"every_save"</span><span class="token punctuation">,</span>
    hub_model_id<span class="token operator">=</span>repository_id<span class="token punctuation">,</span>
    hub_token<span class="token operator">=</span>HfFolder<span class="token punctuation">.</span>get_token<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># Trainer</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>val_dataset<span class="token punctuation">,</span>
<span class="token punctuation">)</span></code></pre>
<p>We can start the training process by running:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Fine-tune the model</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<p>Evaluate the model:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Evaluate the model</span>
trainer<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<hr>
<h2>Publishing</h2>
<p>We are ready to publish our model to Hugging Face 🤗</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Save our tokenizer and create a model card</span>
tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>repository_id<span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>create_model_card<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># Push the results to the hub</span>
trainer<span class="token punctuation">.</span>push_to_hub<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<h2>Test The Model</h2>
<p>At this point, our model should have been published and will be available for use. Let’s test it!</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># TEST MODEL</span>

<span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">'text-classification'</span><span class="token punctuation">,</span>repository_id<span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his innocence and vowing: quot;After the crucifixion comes the resurrection. quot; .."</span>
result <span class="token operator">=</span> classifier<span class="token punctuation">(</span>text<span class="token punctuation">)</span>

predicted_label <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Predicted label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>predicted_label<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></code></pre>
<h2>Congratulations</h2>
<p>You have fine-tuned and published a RoBERTa model for text classification using Hugging Face 🤗 transformers and datasets libraries!</p>
<p><a href="https://huggingface.co/achimoraites/roberta-base_ag_news" rel="nofollow"><strong>For reference, here is my fine-tuned model on Hugging Face</strong> 🤗</a></p>
<p><a href="https://github.com/achimoraites/machine-learning-playground/blob/main/NLP/Text%20classification/RoBERTa_Finetuning.ipynb" rel="nofollow"><strong>You can find the code here</strong></a></p>
<p>Also published <a href="https://medium.com/@achillesmoraites/fine-tuning-roberta-for-topic-classification-with-hugging-face-transformers-and-datasets-library-c6f8432d0820" rel="nofollow">on medium</a></p>
<p>Happy 🤖 learning 😀!</p>

	</article></main>

<footer class="bg-gray-900 mt-8"><hr class="border-1 border-gray-600 w-full">

<p class="text-gray-400 py-3 text-xs text-center">© 2021 <b><a rel="noopener noreferrer" target="_blank" href="https://achimoraites.github.io">Achilles Moraites</a></b></p>
</footer>



	
</div>
	</body>
</html>
